#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Original vs Recovered Data Duplicate Checker
Orijinal veri seti ile kurtarÄ±lan backup verilerini karÅŸÄ±laÅŸtÄ±rÄ±r
Duplicate'leri tespit eder ve temiz augmented veri Ã¼retir
"""

import json
import hashlib
import os
from datetime import datetime
from typing import List, Dict, Set, Tuple
from difflib import SequenceMatcher

class OriginalVsRecoveredChecker:
    """Orijinal ve kurtarÄ±lan veri karÅŸÄ±laÅŸtÄ±rma sÄ±nÄ±fÄ±"""
    
    def __init__(self):
        self.stats = {
            'original_count': 0,
            'recovered_count': 0,
            'exact_duplicates': 0,
            'similar_duplicates': 0,
            'unique_augmented': 0,
            'total_checked': 0
        }
        self.similarity_threshold = 0.85  # %85 benzerlik
        
    def log(self, message: str, level: str = "INFO"):
        """Log mesajÄ±"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] {level}: {message}")
        
    def get_qa_hash(self, qa_pair: Dict) -> str:
        """Q&A Ã§ifti iÃ§in exact hash"""
        soru = qa_pair.get('soru', '').strip().lower()
        cevap = qa_pair.get('cevap', '').strip().lower()
        text = f"{soru}|{cevap}"
        return hashlib.md5(text.encode('utf-8')).hexdigest()
        
    def get_content_fingerprint(self, qa_pair: Dict) -> str:
        """Ä°Ã§erik iÃ§in daha esnek fingerprint (citation'lar hariÃ§)"""
        soru = qa_pair.get('soru', '').strip().lower()
        cevap = qa_pair.get('cevap', '').strip().lower()
        
        # Citation'larÄ± temizle
        import re
        cevap = re.sub(r'\[cite[:\s]*\d+[,\s\d]*\]', '', cevap)
        cevap = re.sub(r'\[cite_start\]', '', cevap)
        
        # Noktalama ve boÅŸluklarÄ± normalize et
        soru = re.sub(r'[^\w\s]', '', soru)
        cevap = re.sub(r'[^\w\s]', '', cevap)
        
        text = f"{soru}|{cevap}"
        return hashlib.md5(text.encode('utf-8')).hexdigest()
        
    def calculate_similarity(self, qa1: Dict, qa2: Dict) -> float:
        """Ä°ki Q&A Ã§ifti arasÄ±nda benzerlik hesapla"""
        soru1 = qa1.get('soru', '').strip().lower()
        cevap1 = qa1.get('cevap', '').strip().lower()
        
        soru2 = qa2.get('soru', '').strip().lower()
        cevap2 = qa2.get('cevap', '').strip().lower()
        
        # Soru benzerliÄŸi
        soru_similarity = SequenceMatcher(None, soru1, soru2).ratio()
        
        # Cevap benzerliÄŸi (citation'lar hariÃ§)
        import re
        cevap1_clean = re.sub(r'\[cite[:\s]*\d+[,\s\d]*\]', '', cevap1)
        cevap2_clean = re.sub(r'\[cite[:\s]*\d+[,\s\d]*\]', '', cevap2)
        
        cevap_similarity = SequenceMatcher(None, cevap1_clean, cevap2_clean).ratio()
        
        # AÄŸÄ±rlÄ±klÄ± ortalama (soru %40, cevap %60)
        overall_similarity = (soru_similarity * 0.4) + (cevap_similarity * 0.6)
        
        return overall_similarity
        
    def load_original_data(self, file_path: str) -> List[Dict]:
        """Orijinal veri setini yÃ¼kle"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if not isinstance(data, list):
                raise ValueError("Orijinal veri list formatÄ±nda deÄŸil")
                
            self.stats['original_count'] = len(data)
            self.log(f"âœ… Orijinal veri yÃ¼klendi: {len(data):,} Q&A Ã§ifti")
            return data
            
        except Exception as e:
            self.log(f"âŒ Orijinal veri yÃ¼klenemedi: {e}", "ERROR")
            return []
            
    def load_recovered_data(self, file_path: str) -> List[Dict]:
        """KurtarÄ±lan veriyi yÃ¼kle"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            if not isinstance(data, list):
                raise ValueError("KurtarÄ±lan veri list formatÄ±nda deÄŸil")
                
            self.stats['recovered_count'] = len(data)
            self.log(f"âœ… KurtarÄ±lan veri yÃ¼klendi: {len(data):,} Q&A Ã§ifti")
            return data
            
        except Exception as e:
            self.log(f"âŒ KurtarÄ±lan veri yÃ¼klenemedi: {e}", "ERROR")
            return []
            
    def find_exact_duplicates(self, original_data: List[Dict], recovered_data: List[Dict]) -> Tuple[Set[str], List[Dict]]:
        """Exact duplicate'leri bul"""
        self.log("\nğŸ” === EXACT DUPLICATE KONTROLÃœ ===")
        
        # Orijinal veri hash'leri
        original_hashes = set()
        for qa in original_data:
            hash_val = self.get_qa_hash(qa)
            original_hashes.add(hash_val)
            
        # KurtarÄ±lan veride exact duplicate'leri bul
        exact_duplicates = set()
        clean_recovered = []
        
        for qa in recovered_data:
            hash_val = self.get_qa_hash(qa)
            if hash_val in original_hashes:
                exact_duplicates.add(hash_val)
            else:
                clean_recovered.append(qa)
                
        self.stats['exact_duplicates'] = len(exact_duplicates)
        self.log(f"ğŸ”„ Exact duplicate: {len(exact_duplicates):,} adet")
        self.log(f"âœ… Temiz kalan: {len(clean_recovered):,} adet")
        
        return exact_duplicates, clean_recovered
        
    def find_similar_duplicates(self, original_data: List[Dict], clean_recovered: List[Dict]) -> Tuple[List[Dict], List[Dict]]:
        """Benzer duplicate'leri bul (%85+ benzerlik)"""
        self.log(f"\nğŸ” === BENZERLÄ°K KONTROLÃœ (%{self.similarity_threshold*100:.0f}+ benzerlik) ===")
        
        similar_pairs = []
        ultra_clean_recovered = []
        
        self.log("ğŸ“Š Benzerlik analizi yapÄ±lÄ±yor...")
        total_comparisons = len(clean_recovered) * len(original_data)
        processed = 0
        
        for i, recovered_qa in enumerate(clean_recovered):
            is_similar = False
            max_similarity = 0
            best_match = None
            
            for original_qa in original_data:
                similarity = self.calculate_similarity(recovered_qa, original_qa)
                
                if similarity > max_similarity:
                    max_similarity = similarity
                    best_match = original_qa
                    
                if similarity >= self.similarity_threshold:
                    is_similar = True
                    break
                    
            processed += len(original_data)
            if (i + 1) % 1000 == 0:
                progress = (processed / total_comparisons) * 100
                self.log(f"   ğŸ“ˆ Ä°lerleme: %{progress:.1f} ({i+1:,}/{len(clean_recovered):,})")
                
            if is_similar:
                similar_pairs.append({
                    'recovered': recovered_qa,
                    'original': best_match,
                    'similarity': max_similarity
                })
            else:
                ultra_clean_recovered.append(recovered_qa)
                
        self.stats['similar_duplicates'] = len(similar_pairs)
        self.stats['unique_augmented'] = len(ultra_clean_recovered)
        
        self.log(f"ğŸ”„ Benzer duplicate: {len(similar_pairs):,} adet")
        self.log(f"âœ… Benzersiz augmented: {len(ultra_clean_recovered):,} adet")
        
        return similar_pairs, ultra_clean_recovered
        
    def create_detailed_report(self, exact_duplicates: Set[str], similar_pairs: List[Dict], 
                              original_data: List[Dict], recovered_data: List[Dict], 
                              ultra_clean_data: List[Dict]) -> Dict:
        """DetaylÄ± rapor oluÅŸtur"""
        
        # Benzerlik daÄŸÄ±lÄ±mÄ±
        similarity_distribution = {'90-100%': 0, '85-90%': 0}
        for pair in similar_pairs:
            sim = pair['similarity']
            if sim >= 0.9:
                similarity_distribution['90-100%'] += 1
            elif sim >= 0.85:
                similarity_distribution['85-90%'] += 1
                
        report = {
            "analysis_timestamp": datetime.now().isoformat(),
            "input_summary": {
                "original_data_count": len(original_data),
                "recovered_data_count": len(recovered_data),
                "total_analyzed": len(original_data) + len(recovered_data)
            },
            "duplicate_analysis": {
                "exact_duplicates": len(exact_duplicates),
                "similar_duplicates": len(similar_pairs),
                "similarity_threshold": f"{self.similarity_threshold*100:.0f}%",
                "similarity_distribution": similarity_distribution
            },
            "final_results": {
                "original_data_count": len(original_data),
                "unique_augmented_count": len(ultra_clean_data),
                "total_final_dataset": len(original_data) + len(ultra_clean_data),
                "multiplication_factor": (len(original_data) + len(ultra_clean_data)) / len(original_data)
            },
            "data_quality": {
                "duplicate_removal_rate": f"{((len(exact_duplicates) + len(similar_pairs)) / len(recovered_data) * 100):.1f}%",
                "augmentation_success_rate": f"{(len(ultra_clean_data) / len(recovered_data) * 100):.1f}%"
            },
            "sample_similar_pairs": similar_pairs[:5] if similar_pairs else [],  # Ä°lk 5 Ã¶rnek
            "recommendations": []
        }
        
        # Ã–neriler
        if len(exact_duplicates) > 100:
            report["recommendations"].append("âš ï¸ Ã‡ok sayÄ±da exact duplicate - augmentation algoritmasÄ± kontrol edilmeli")
            
        if len(similar_pairs) > len(ultra_clean_data):
            report["recommendations"].append("âš ï¸ Benzer duplicate'ler fazla - yaratÄ±cÄ±lÄ±k artÄ±rÄ±lmalÄ±")
        else:
            report["recommendations"].append("âœ… Ä°yi augmentation kalitesi")
            
        if len(ultra_clean_data) < len(original_data) * 0.5:
            report["recommendations"].append("âš ï¸ DÃ¼ÅŸÃ¼k augmentation verimi - parametreler gÃ¶zden geÃ§irilmeli")
        else:
            report["recommendations"].append("âœ… BaÅŸarÄ±lÄ± augmentation verimi")
            
        return report
        
    def run_full_analysis(self, original_file: str, recovered_file: str) -> Tuple[List[Dict], List[Dict], Dict]:
        """Tam analiz sÃ¼recini Ã§alÄ±ÅŸtÄ±r"""
        self.log("ğŸš€ === DUPLICATE ANALYSIS BAÅLATILDI ===")
        
        # 1. Verileri yÃ¼kle
        original_data = self.load_original_data(original_file)
        recovered_data = self.load_recovered_data(recovered_file)
        
        if not original_data or not recovered_data:
            self.log("âŒ Veri yÃ¼kleme baÅŸarÄ±sÄ±z!", "ERROR")
            return [], [], {}
            
        # 2. Exact duplicate kontrolÃ¼
        exact_duplicates, clean_recovered = self.find_exact_duplicates(original_data, recovered_data)
        
        # 3. Benzerlik kontrolÃ¼
        similar_pairs, ultra_clean_recovered = self.find_similar_duplicates(original_data, clean_recovered)
        
        # 4. Final veri seti oluÅŸtur
        final_dataset = original_data + ultra_clean_recovered
        
        # 5. Rapor oluÅŸtur
        report = self.create_detailed_report(exact_duplicates, similar_pairs, original_data, 
                                           recovered_data, ultra_clean_recovered)
        
        self.log(f"\nğŸ‰ === ANALÄ°Z TAMAMLANDI ===")
        self.log(f"ğŸ“Š Orijinal: {len(original_data):,}")
        self.log(f"ğŸ†• Benzersiz augmented: {len(ultra_clean_recovered):,}")
        self.log(f"ğŸ“ˆ Final toplam: {len(final_dataset):,}")
        self.log(f"ğŸ”¢ Ã‡arpan: {len(final_dataset) / len(original_data):.2f}x")
        
        return final_dataset, ultra_clean_recovered, report

def main():
    """Ana fonksiyon"""
    print("ğŸ” Original vs Recovered Duplicate Checker")
    print("=" * 50)
    
    # Dosya yollarÄ±nÄ± belirle
    original_file = "250630AllData.json"  # Orijinal veri seti
    
    # En yeni recovered dosyasÄ±nÄ± bul
    import glob
    recovered_files = glob.glob("output/recovered_data_*.json")
    if not recovered_files:
        print("âŒ KurtarÄ±lan veri dosyasÄ± bulunamadÄ±!")
        print("   Ã–nce backup_recovery_script.py Ã§alÄ±ÅŸtÄ±rÄ±n.")
        return
        
    recovered_file = max(recovered_files)  # En yeni dosya
    print(f"ğŸ“ Orijinal dosya: {original_file}")
    print(f"ğŸ“ KurtarÄ±lan dosya: {recovered_file}")
    
    # Dosya varlÄ±ÄŸÄ±nÄ± kontrol et
    if not os.path.exists(original_file):
        print(f"âŒ Orijinal dosya bulunamadÄ±: {original_file}")
        return
        
    if not os.path.exists(recovered_file):
        print(f"âŒ KurtarÄ±lan dosya bulunamadÄ±: {recovered_file}")
        return
        
    # Analiz yap
    checker = OriginalVsRecoveredChecker()
    final_dataset, clean_augmented, report = checker.run_full_analysis(original_file, recovered_file)
    
    if not final_dataset:
        print("âŒ Analiz baÅŸarÄ±sÄ±z!")
        return
        
    # SonuÃ§larÄ± kaydet
    os.makedirs('output', exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M')
    
    # Final temiz veri seti
    final_file = f'output/final_clean_dataset_{timestamp}.json'
    with open(final_file, 'w', encoding='utf-8') as f:
        json.dump(final_dataset, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ Final temiz veri seti: {final_file}")
    
    # Sadece augmented veriler
    augmented_file = f'output/clean_augmented_only_{timestamp}.json'
    with open(augmented_file, 'w', encoding='utf-8') as f:
        json.dump(clean_augmented, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ Temiz augmented veriler: {augmented_file}")
    
    # Analiz raporu
    report_file = f'output/duplicate_analysis_report_{timestamp}.json'
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“‹ Duplicate analiz raporu: {report_file}")
    
    # Ã–zet rapor
    print(f"\nğŸ“Š === Ã–ZET RAPOR ===")
    print(f"ğŸ“¥ Orijinal veri: {report['input_summary']['original_data_count']:,}")
    print(f"ğŸ“¦ KurtarÄ±lan veri: {report['input_summary']['recovered_data_count']:,}")
    print(f"ğŸ”„ Exact duplicate: {report['duplicate_analysis']['exact_duplicates']:,}")
    print(f"ğŸ”„ Benzer duplicate: {report['duplicate_analysis']['similar_duplicates']:,}")
    print(f"âœ… Benzersiz augmented: {report['final_results']['unique_augmented_count']:,}")
    print(f"ğŸ“ˆ Final toplam: {report['final_results']['total_final_dataset']:,}")
    print(f"ğŸ”¢ Ã‡arpan: {report['final_results']['multiplication_factor']:.2f}x")
    print(f"ğŸ“Š Augmentation baÅŸarÄ±: {report['data_quality']['augmentation_success_rate']}")
    
    print(f"\nâœ… Duplicate temizliÄŸi tamamlandÄ±!")

if __name__ == "__main__":
    main() 