#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BASIT VE HIZLI VERÄ° BÄ°RLEÅTÄ°RME VE KARÅILAÅTIRMA
650 batch'e kadar olan backup'larÄ± al, orijinalle karÅŸÄ±laÅŸtÄ±r
"""

import json
import glob
from difflib import SequenceMatcher

def load_json(file_path):
    """JSON dosyasÄ± yÃ¼kle"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except:
        return []

def text_similarity(a, b):
    """Ä°ki metin arasÄ±ndaki benzerlik %"""
    return SequenceMatcher(None, a.lower(), b.lower()).ratio()

def quick_merge_and_check():
    """HÄ±zlÄ± birleÅŸtirme ve kontrol"""
    print("ğŸš€ HIZLI VERÄ° BÄ°RLEÅTÄ°RME BAÅLADI")
    
    # 1. Orijinal veriyi yÃ¼kle
    print("ğŸ“‚ Orijinal veri yÃ¼kleniyor...")
    original_data = load_json('250630AllData.json')
    print(f"âœ… Orijinal: {len(original_data):,} Q&A")
    
    # 2. Backup dosyalarÄ±nÄ± bul (650'ye kadar)
    backup_files = []
    for i in range(0, 651, 50):  # 0, 50, 100, 150, ... 650
        pattern = f'backups/backup_batch_{i}_*.json'
        files = glob.glob(pattern)
        if files:
            backup_files.extend(files)
    
    backup_files.sort()
    print(f"ğŸ“ {len(backup_files)} backup dosyasÄ± bulundu")
    
    # 3. Backup'larÄ± birleÅŸtir
    print("ğŸ”„ Backup'lar birleÅŸtiriliyor...")
    all_backup_data = []
    for file_path in backup_files:
        data = load_json(file_path)
        all_backup_data.extend(data)
        print(f"   ğŸ“„ {file_path}: {len(data):,} veri")
    
    print(f"âœ… Toplam backup: {len(all_backup_data):,} Q&A")
    
    # 4. Basit duplicate temizleme (backup iÃ§inde)
    print("ğŸ§¹ Backup iÃ§i duplicate temizleme...")
    seen_texts = set()
    clean_backup = []
    
    for qa in all_backup_data:
        qa_text = qa.get('soru', '') + qa.get('cevap', '')
        if qa_text not in seen_texts:
            seen_texts.add(qa_text)
            clean_backup.append(qa)
    
    print(f"âœ… Temiz backup: {len(clean_backup):,} Q&A")
    print(f"ğŸ—‘ï¸ KaldÄ±rÄ±lan duplicate: {len(all_backup_data) - len(clean_backup):,}")
    
    # 5. Orijinal ile karÅŸÄ±laÅŸtÄ±rma
    print("ğŸ” Orijinal ile benzerlik kontrolÃ¼...")
    similar_count = 0
    unique_backup = []
    
    for i, backup_qa in enumerate(clean_backup):
        if i % 1000 == 0:
            print(f"   ğŸ“Š Ä°lerleme: {i:,}/{len(clean_backup):,}")
        
        is_similar = False
        backup_text = backup_qa.get('soru', '') + ' ' + backup_qa.get('cevap', '')
        
        for orig_qa in original_data:
            orig_text = orig_qa.get('soru', '') + ' ' + orig_qa.get('cevap', '')
            
            if text_similarity(backup_text, orig_text) > 0.85:
                similar_count += 1
                is_similar = True
                break
        
        if not is_similar:
            unique_backup.append(backup_qa)
    
    # 6. SonuÃ§lar
    print("\nğŸ¯ === SONUÃ‡LAR ===")
    print(f"ğŸ“Š Orijinal veri: {len(original_data):,}")
    print(f"ğŸ“Š Backup veri (temiz): {len(clean_backup):,}")
    print(f"ğŸ”„ Benzer olanlar: {similar_count:,}")
    print(f"âœ¨ Benzersiz yeni: {len(unique_backup):,}")
    print(f"ğŸ‰ TOPLAM: {len(original_data) + len(unique_backup):,}")
    
    # 7. Final birleÅŸtirme
    final_dataset = original_data + unique_backup
    
    # 8. Kaydet
    output_file = f'final_merged_dataset_{len(final_dataset)}.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_dataset, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ Kaydedildi: {output_file}")
    print("âœ… TAMAMLANDI!")
    
    return len(original_data), len(unique_backup), len(final_dataset)

if __name__ == "__main__":
    quick_merge_and_check() 