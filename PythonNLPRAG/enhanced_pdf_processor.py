#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ultra Safe PDF to Q&A Dataset Generator
Enhanced PDF processing system for ML training data generation
"""

import json
import time
import asyncio
import logging
import os
import gc
import threading
import signal
import sys
import re
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Set, Tuple
from pathlib import Path
import fitz  # PyMuPDF
import google.generativeai as genai
from PIL import Image
import io

# Import working modules
from pdf_api_manager import APIKeyManager

class UltraSafePDFProcessor:
    """Ultra g√ºvenli PDF i≈üleme sistemi - ML eƒüitimi i√ßin optimize edilmi≈ü"""
    
    def __init__(self, config_path="config.json"):
        print("üõ°Ô∏è Ultra Safe PDF Processor ba≈ülatƒ±lƒ±yor...")
        
        # Config y√ºkle ve doƒürula
        self.load_and_validate_config(config_path)
        
        # Logging sistemi
        self.setup_logging()
        
        # Core componentler
        self.setup_core_systems()
        
        # API sistemleri
        self.setup_apis()
        
        # Signal handlers
        self.setup_signal_handlers()
        
        # Stats
        self.stats = {
            'total_pdfs_processed': 0,
            'total_questions_generated': 0,
            'successful_pages': 0,
            'failed_pages': 0,
            'api_failures': 0,
            'start_time': datetime.now().isoformat(),
            'categories_distribution': {},
            'difficulty_distribution': {},
            'avg_quality_score': 0.0
        }
        
        # PDF specific
        self.processed_files = self._get_already_processed_files()
        
        self.logger.info("üõ°Ô∏è Ultra Safe PDF Processor hazƒ±r!")
        
    def load_and_validate_config(self, config_path: str):
        """Enhanced config loading with validation"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                self.config = json.load(f)
                
            # Basic validation
            required_keys = ['api_keys', 'pdf_folder', 'output_folder', 'output_filename']
            for key in required_keys:
                if key not in self.config:
                    raise ValueError(f"Missing required config key: {key}")
                    
            # Config dosyasƒ± bilgilerini sakla
            self.config_file_path = config_path
            self.config_last_modified = os.path.getmtime(config_path)
                
        except Exception as e:
            print(f"‚ùå Config hatasƒ±: {e}")
            sys.exit(1)
            
    def setup_logging(self):
        """Enhanced logging system"""
        os.makedirs('logs', exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f'logs/enhanced_pdf_processor_{datetime.now().strftime("%Y%m%d_%H%M")}.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        
    def setup_core_systems(self):
        """Core systems setup"""
        # Create necessary directories
        os.makedirs('checkpoints', exist_ok=True)
        os.makedirs('logs', exist_ok=True)
        os.makedirs('output', exist_ok=True)
        os.makedirs(self.config.get('output_folder', 'output_json'), exist_ok=True)
        
        # Emergency stop file
        self.emergency_stop_file = 'EMERGENCY_STOP_PDF'
        if os.path.exists(self.emergency_stop_file):
            os.remove(self.emergency_stop_file)
            
        self.logger.info("üõ°Ô∏è Core systems initialized")
            
    def setup_apis(self):
        """API systems setup"""
        self.api_manager = APIKeyManager(self.config['api_keys'], self.logger)
        
        # Test all keys
        active_count = self.api_manager.test_all_keys()
        
        if active_count == 0:
            raise RuntimeError("‚ùå Hi√ßbir API key √ßalƒ±≈ümƒ±yor!")
            
        self.logger.info(f"‚úÖ {active_count}/{len(self.config['api_keys'])} API key aktif")
        
        if active_count < 3:
            self.logger.warning(f"‚ö†Ô∏è Sadece {active_count} API key aktif. ƒ∞≈ülem yava≈ü olabilir.")
        
    def setup_signal_handlers(self):
        """Signal handlers for graceful shutdown"""
        def signal_handler(signum, frame):
            self.logger.warning(f"Signal {signum} alƒ±ndƒ±. G√ºvenli shutdown...")
            self.emergency_shutdown()
            sys.exit(0)
            
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
        
    def _get_already_processed_files(self) -> Set[str]:
        """Get already processed files from checkpoint"""
        try:
            with open('checkpoints/processed_files.json', 'r') as f:
                return set(json.load(f))
        except:
            return set()

    def create_ml_optimized_prompt(self) -> str:
        """ML training i√ßin optimize edilmi≈ü prompt"""
        max_questions = self.config.get('max_questions_per_pdf', 20)
        
        return f"""
Sen, makine √∂ƒürenmesi modelleri i√ßin profesyonel seviyede eƒüitim verisi √ºreten uzman bir AI sistemisin.

**G√ñREV**: PDF i√ßeriƒüini analiz ederek, farklƒ± kategorilerde ve zorluk seviyelerinde y√ºksek kaliteli soru-cevap √ßiftleri √ºret.

**KATEGORƒ∞LER** (Her kategoriden minimum 2 soru):
1. **Faktuel Bilgi**: Tanƒ±mlar, sayƒ±sal veriler, spesifik bilgiler
2. **Kavramsal Anlama**: A√ßƒ±klamalar, sebep-sonu√ß ili≈ükileri, kar≈üƒ±la≈ütƒ±rmalar
3. **Analitik D√º≈ü√ºnme**: √áƒ±karƒ±mlar, analizler, deƒüerlendirmeler
4. **Uygulama**: Pratik kullanƒ±m, √∂rnekler, problem √ß√∂zme
5. **Ele≈ütirel Deƒüerlendirme**: Avantaj-dezavantajlar, kritik deƒüerlendirme

**ZORLUK SEVƒ∞YELERƒ∞**:
- **Temel**: Doƒürudan metinden alƒ±nabilir (30%)
- **Orta**: Hafif √ßƒ±karƒ±m gerektirir (50%)
- **ƒ∞leri**: Derin analiz gerektirir (20%)

**KALƒ∞TE KURALLARI**:
‚úÖ Sorular 20-150 karakter, net ve spesifik
‚úÖ Cevaplar 100-400 kelime, detaylƒ± ve yapƒ±landƒ±rƒ±lmƒ±≈ü
‚úÖ Bilimsel doƒüruluk ve kesinlik
‚úÖ √ñrneklerle desteklenmi≈ü a√ßƒ±klamalar
‚úÖ Belirsiz ifadeler yasak ("muhtemelen", "genellikle")
‚úÖ Tablo/≈üekil referanslarƒ± yasak

**√áIKTI FORMATI** (Kesinlikle bu format):
```json
[
  {{
    "soru": "A√ßƒ±k ve spesifik soru metni",
    "cevap": "Detaylƒ±, yapƒ±landƒ±rƒ±lmƒ±≈ü cevap (100-400 kelime)",
    "kategori": "Faktuel Bilgi",
    "zorluk": "Temel",
    "anahtar_kelimeler": ["anahtar1", "anahtar2", "anahtar3"],
    "kaynak_tipi": "metin"
  }}
]
```

**HEDEF**: {max_questions} adet benzersiz, y√ºksek kaliteli soru-cevap √ßifti √ºret.
Her soru farklƒ± bir a√ßƒ±dan konuyu ele almalƒ±.
"""

    def validate_qa_quality(self, qa_data: Dict) -> Tuple[bool, int, List[str]]:
        """Q&A kalitesini deƒüerlendir"""
        quality_score = 0
        issues = []
        
        if not isinstance(qa_data, dict):
            return False, 0, ["Invalid format"]
            
        # Required fields check
        required_fields = ['soru', 'cevap', 'kategori', 'zorluk', 'anahtar_kelimeler', 'kaynak_tipi']
        missing_fields = [f for f in required_fields if f not in qa_data]
        if missing_fields:
            return False, 0, [f"Missing fields: {missing_fields}"]
        
        question = qa_data['soru'].strip()
        answer = qa_data['cevap'].strip()
        
        # Length checks
        if 20 <= len(question) <= 150:
            quality_score += 15
        else:
            issues.append(f"Question length issue: {len(question)} chars")
            
        word_count = len(answer.split())
        if 100 <= len(answer) <= 1600 and word_count >= 15:  # Character and word count
            quality_score += 25
        else:
            issues.append(f"Answer length issue: {len(answer)} chars, {word_count} words")
        
        # Content quality checks
        forbidden_patterns = [
            r'tablo\s*\d+', r'≈üekil\s*\d+', r'grafik\s*\d+',
            r'yukarƒ±daki\s+(tablo|≈üekil|grafik)', r'a≈üaƒüƒ±daki\s+(tablo|≈üekil|grafik)'
        ]
        
        full_text = (question + " " + answer).lower()
        if not any(re.search(pattern, full_text) for pattern in forbidden_patterns):
            quality_score += 20
        else:
            issues.append("Contains forbidden table/figure references")
        
        # Category validation
        valid_categories = ['Faktuel Bilgi', 'Kavramsal Anlama', 'Analitik D√º≈ü√ºnme', 'Uygulama', 'Ele≈ütirel Deƒüerlendirme']
        if qa_data['kategori'] in valid_categories:
            quality_score += 10
        else:
            issues.append(f"Invalid category: {qa_data['kategori']}")
        
        # Difficulty validation
        valid_difficulties = ['Temel', 'Orta', 'ƒ∞leri']
        if qa_data['zorluk'] in valid_difficulties:
            quality_score += 10
        else:
            issues.append(f"Invalid difficulty: {qa_data['zorluk']}")
        
        # Keywords validation
        keywords = qa_data.get('anahtar_kelimeler', [])
        if isinstance(keywords, list) and len(keywords) >= 2:
            quality_score += 10
        else:
            issues.append("Insufficient keywords")
        
        # Vague language check
        vague_patterns = ['muhtemelen', 'sanƒ±rƒ±m', 'galiba', 'belki', '√ßoƒüunlukla', 'genellikle']
        vague_count = sum(1 for pattern in vague_patterns if pattern in answer.lower())
        if vague_count == 0:
            quality_score += 10
        elif vague_count > 2:
            issues.append(f"Too much vague language: {vague_count}")
        
        return quality_score >= 70, quality_score, issues

    async def process_with_gemini(self, text_content: str, images: List[bytes]) -> Optional[List[Dict]]:
        """Enhanced Gemini API processing"""
        start_time = time.time()
        
        try:
            # Get best model
            model_info = self.api_manager.get_best_model()
            if not model_info:
                self.logger.error("No available API models")
                return None
            
            model = model_info['model']
            prompt = self.create_ml_optimized_prompt()
            
            # Prepare content
            content_parts = [prompt, text_content]
            
            # Add images (limit to 5 for performance)
            for i, img_data in enumerate(images[:5]):
                try:
                    pil_img = Image.open(io.BytesIO(img_data))
                    content_parts.append(pil_img)
                except Exception as e:
                    self.logger.warning(f"Image {i} processing failed: {e}")
            
            # Generate response
            response = model.generate_content(
                content_parts,
                generation_config=genai.types.GenerationConfig(
                    candidate_count=1,
                    max_output_tokens=8192,
                    temperature=0.1
                )
            )
            
            if not response.text:
                self.api_manager.record_failure(model_info)
                return None
            
            # Parse and validate response
            try:
                response_text = response.text.strip()
                if response_text.startswith('```json'):
                    response_text = response_text[7:]
                if response_text.endswith('```'):
                    response_text = response_text[:-3]
                response_text = response_text.strip()
                
                qa_pairs = json.loads(response_text)
                
                if not isinstance(qa_pairs, list):
                    self.logger.warning("Response is not a JSON array")
                    return None
                
                # Validate each Q&A pair
                valid_pairs = []
                for qa in qa_pairs:
                    is_valid, score, issues = self.validate_qa_quality(qa)
                    
                    if is_valid:
                        # Add metadata for ML training
                        qa['kalite_skoru'] = score
                        qa['kelime_sayisi'] = len(qa['cevap'].split())
                        qa['karakter_sayisi'] = len(qa['cevap'])
                        valid_pairs.append(qa)
                        
                        # Update stats
                        category = qa.get('kategori', 'Unknown')
                        difficulty = qa.get('zorluk', 'Unknown')
                        self.stats['categories_distribution'][category] = self.stats['categories_distribution'].get(category, 0) + 1
                        self.stats['difficulty_distribution'][difficulty] = self.stats['difficulty_distribution'].get(difficulty, 0) + 1
                    else:
                        self.logger.warning(f"Q&A rejected (score: {score}): {' | '.join(issues)}")
                
                if valid_pairs:
                    self.api_manager.record_success(model_info)
                    
                    # Calculate average quality score
                    if self.stats['total_questions_generated'] > 0:
                        self.stats['avg_quality_score'] = (
                            self.stats['avg_quality_score'] * self.stats['total_questions_generated'] + 
                            sum(qa['kalite_skoru'] for qa in valid_pairs)
                        ) / (self.stats['total_questions_generated'] + len(valid_pairs))
                    else:
                        self.stats['avg_quality_score'] = sum(qa['kalite_skoru'] for qa in valid_pairs) / len(valid_pairs)
                    
                    self.stats['total_questions_generated'] += len(valid_pairs)
                    
                    self.logger.info(f"Generated {len(valid_pairs)} quality Q&A pairs in {time.time() - start_time:.2f}s")
                    return valid_pairs
                else:
                    self.logger.warning("No valid Q&A pairs after quality filtering")
                    return None
                    
            except json.JSONDecodeError as e:
                self.logger.error(f"JSON parsing failed: {e}")
                self.api_manager.record_failure(model_info)
                return None
                
        except Exception as e:
            self.logger.error(f"Gemini API call failed: {e}")
            if model_info:
                self.api_manager.record_failure(model_info)
            return None
            
    def check_emergency_stop(self) -> bool:
        """Emergency stop check"""
        # This method is no longer needed as safety monitoring is removed.
        # Keeping it for now, but it will always return False.
        return False
        
    def emergency_shutdown(self):
        """Emergency shutdown"""
        self.logger.critical("üö® EMERGENCY SHUTDOWN!")
        
        # Save emergency data
        emergency_data = {
            "shutdown_time": datetime.now().isoformat(),
            "stats": self.stats,
            "processed_files": list(self.processed_files),
            "reason": "Emergency shutdown"
        }
        
        with open('emergency_shutdown_pdf.json', 'w', encoding='utf-8') as f:
            json.dump(emergency_data, f, ensure_ascii=False, indent=2)
            
    def save_checkpoint(self):
        """Checkpoint save"""
        checkpoint = {
            "processed_files": list(self.processed_files),
            "stats": self.stats.copy(),
            "timestamp": datetime.now().isoformat()
        }
        
        os.makedirs('checkpoints', exist_ok=True)
        with open('checkpoints/processed_files.json', 'w', encoding='utf-8') as f:
            json.dump(list(self.processed_files), f, ensure_ascii=False, indent=2)
            
        with open('checkpoints/latest_pdf.json', 'w', encoding='utf-8') as f:
            json.dump(checkpoint, f, ensure_ascii=False, indent=2)
            
    def print_progress_report(self):
        """Progress report"""
        start_time = datetime.fromisoformat(self.stats['start_time'])
        elapsed = datetime.now() - start_time
        
        if self.stats['total_pdfs_processed'] > 0:
            success_rate = (self.stats['successful_pages'] / 
                           max(self.stats['successful_pages'] + self.stats['failed_pages'], 1) * 100)
        else:
            success_rate = 0
        
        self.logger.info(f"""
üìä === PDF ƒ∞≈ûLEME RAPORU ===
‚è±Ô∏è  Ge√ßen s√ºre: {elapsed}
üìÑ ƒ∞≈ülenen PDF: {self.stats['total_pdfs_processed']}
‚ùì √úretilen soru: {self.stats['total_questions_generated']}
‚úÖ Ba≈üarƒ±lƒ± sayfa: {self.stats['successful_pages']}
‚ùå Ba≈üarƒ±sƒ±z sayfa: {self.stats['failed_pages']}
üìà Ba≈üarƒ± oranƒ±: %{success_rate:.1f}
üí∞ Maliyet: {self.cost_tracker.get_estimated_cost()}
=========================""")
        
    def extract_pdf_content(self, pdf_path: Path) -> Tuple[str, List[bytes]]:
        """PDF i√ßeriƒüini √ßƒ±kar - enhanced version"""
        try:
            doc = fitz.open(pdf_path)
            text_content = ""
            images = []
            
            for page_num in range(doc.page_count):
                page = doc[page_num]
                
                # Text extraction
                page_text = page.get_text()
                if page_text.strip():
                    text_content += f"\n--- Sayfa {page_num + 1} ---\n"
                    text_content += page_text
                    
                # Image extraction (eƒüer config'de aktifse)
                if self.config.get('extract_images', False):
                    image_list = page.get_images()
                    for img_index, img in enumerate(image_list):
                        try:
                            xref = img[0]
                            pix = fitz.Pixmap(doc, xref)
                            if pix.n - pix.alpha < 4:  # GRAY or RGB
                                img_data = pix.tobytes("png")
                                images.append(img_data)
                            pix = None
                        except:
                            continue
                            
            doc.close()
            
            # Content validation
            if len(text_content.strip()) < 100:
                raise ValueError("√áƒ±karƒ±lan i√ßerik √ßok kƒ±sa")
                
            self.logger.info(f"üìÑ {pdf_path.name}: {len(text_content)} karakter, {len(images)} resim")
            return text_content, images
            
        except Exception as e:
            self.logger.error(f"PDF content extraction hatasƒ±: {e}")
            raise

    async def process_single_pdf(self, pdf_path: Path) -> bool:
        """Tek PDF i≈üleme - enhanced version"""
        try:
            # Skip if already processed
            if str(pdf_path) in self.processed_files:
                self.logger.info(f"‚è≠Ô∏è Atlanƒ±yor (i≈ülenmi≈ü): {pdf_path.name}")
                return True
                
            self.logger.info(f"üìÑ PDF i≈üleniyor: {pdf_path.name}")
            
            # Extract content
            text_content, images = self.extract_pdf_content(pdf_path)
            
            # Process with Gemini
            qa_pairs = await self.process_with_gemini(text_content, images)
            
            if not qa_pairs:
                self.logger.warning(f"‚ùå {pdf_path.name}: Soru-cevap √ºretilemedi")
                return False
                
            # Save results (Enhanced JSONL format)
            output_file = Path(self.config['output_folder']) / self.config['output_filename']
            
            with open(output_file, 'a', encoding='utf-8') as f:
                for qa in qa_pairs:
                    # Enhanced ML training format
                    ml_format = {
                        'soru': qa['soru'],
                        'cevap': qa['cevap'],
                        'kategori': qa['kategori'],
                        'zorluk': qa['zorluk'],
                        'anahtar_kelimeler': qa['anahtar_kelimeler'],
                        'kaynak_tipi': qa['kaynak_tipi'],
                        'kalite_skoru': qa['kalite_skoru'],
                        'kelime_sayisi': qa['kelime_sayisi'],
                        'karakter_sayisi': qa['karakter_sayisi'],
                        'kaynak_dosya': pdf_path.stem,
                        'uretim_tarihi': datetime.now().isoformat(),
                        'model_versiyonu': self.config.get('model_name', 'gemini-1.5-flash-latest'),
                        'processor_version': '3.0_enhanced_ml'
                    }
                    
                    f.write(json.dumps(ml_format, ensure_ascii=False) + '\n')
                     
            # Update stats
            self.stats['total_pdfs_processed'] += 1
            
            # Mark as processed
            self.processed_files.add(str(pdf_path))
            
            self.logger.info(f"‚úÖ {pdf_path.name}: {len(qa_pairs)} soru √ºretildi")
            return True
                 
        except Exception as e:
            self.logger.error(f"‚ùå PDF i≈üleme hatasƒ± {pdf_path.name}: {e}")
            self.stats['failed_pages'] += 1
            return False

    async def save_final_results(self):
        """Final results kaydet - cleaned version"""
        os.makedirs('output', exist_ok=True)
        
        end_time = datetime.now()
        start_time = datetime.fromisoformat(self.stats['start_time'])
        
        final_report = {
            "timestamp": end_time.isoformat(),
            "processing_duration": str(end_time - start_time),
            "summary": {
                "total_pdfs": self.stats['total_pdfs_processed'],
                "total_questions": self.stats['total_questions_generated'],
                "success_rate": round(
                    self.stats['successful_pages'] / 
                    max(self.stats['successful_pages'] + self.stats['failed_pages'], 1) * 100),
                "avg_quality_score": round(self.stats['avg_quality_score'], 2),
                "categories_distribution": self.stats['categories_distribution'],
                "difficulty_distribution": self.stats['difficulty_distribution']
            },
            "detailed_stats": self.stats,
            "api_status": self.api_manager.get_status_report(),
            "processed_files": list(self.processed_files)
        }
        
        # Save comprehensive report
        with open('output/enhanced_processing_report.json', 'w', encoding='utf-8') as f:
            json.dump(final_report, f, ensure_ascii=False, indent=2)
            
        self.logger.info("üìä Final rapor kaydedildi: output/enhanced_processing_report.json")

    async def process_all_pdfs(self):
        """T√ºm PDF'leri i≈üle - enhanced main function"""
        try:
            self.logger.info("üöÄ Enhanced PDF Processing ba≈ülatƒ±ldƒ±")
            
            # PDF dosyalarƒ±nƒ± bul
            pdf_folder = Path(self.config['pdf_folder'])
            pdf_files = list(pdf_folder.glob('*.pdf'))
            
            if not pdf_files:
                self.logger.warning(f"‚ùå {pdf_folder} klas√∂r√ºnde PDF bulunamadƒ±!")
                return
                
            self.logger.info(f"üìÅ {len(pdf_files)} PDF dosyasƒ± bulundu")
            
            successful_count = 0
            failed_count = 0
            
            for i, pdf_path in enumerate(pdf_files):
                self.logger.info(f"üìä Progress: {i+1}/{len(pdf_files)} - {pdf_path.name}")
                
                # Process PDF
                success = await self.process_single_pdf(pdf_path)
                
                if success:
                    successful_count += 1
                else:
                    failed_count += 1
                
                # Checkpoint (her 10 dosyada bir)
                if (i + 1) % 10 == 0:
                    self.save_checkpoint()
                    
                # Memory cleanup
                if (i + 1) % 5 == 0:
                    gc.collect()
                    self.print_progress_report()
                    
                # Rate limiting between files
                await asyncio.sleep(2)
                     
            # Final stats update
            self.stats['successful_pages'] = successful_count
            self.stats['failed_pages'] = failed_count
            
            # Save final results
            await self.save_final_results()
            
            self.logger.info(f"""
üéâ === ENHANCED PDF ƒ∞≈ûLEME TAMAMLANDI! ===
üìÑ ƒ∞≈ülenen PDF: {successful_count}
‚ùå Ba≈üarƒ±sƒ±z: {failed_count}
‚ùì Toplam soru: {self.stats['total_questions_generated']}
‚≠ê Ortalama kalite: {self.stats['avg_quality_score']:.1f}
üìà Ba≈üarƒ± oranƒ±: %{(successful_count/(successful_count+failed_count)*100) if (successful_count+failed_count) > 0 else 0:.1f}
üìä Kategori daƒüƒ±lƒ±mƒ±: {self.stats['categories_distribution']}
üìà Zorluk daƒüƒ±lƒ±mƒ±: {self.stats['difficulty_distribution']}
========================================""")
                  
        except Exception as e:
            self.logger.critical(f"‚ùå Kritik hata: {e}")
            self.emergency_shutdown()
            raise

# Main function
async def main():
    """Enhanced main function"""
    try:
        processor = UltraSafePDFProcessor()
        await processor.process_all_pdfs()
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è ƒ∞≈ülem kullanƒ±cƒ± tarafƒ±ndan durduruldu")
    except Exception as e:
        print(f"‚ùå Fatal error: {e}")
        
if __name__ == "__main__":
    asyncio.run(main()) 