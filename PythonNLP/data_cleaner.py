#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Veri Temizleme ModÃ¼lÃ¼
JSON dosyasÄ±ndaki soru-cevap Ã§iftlerini analiz eder ve kalitesiz verileri temizler.
"""

import json
import re
import os
from typing import List, Dict, Tuple
from collections import Counter

class DataCleaner:
    def __init__(self):
        """Veri temizleyici sÄ±nÄ±fÄ±nÄ± baÅŸlatÄ±r"""
        self.quality_rules = {
            'min_question_length': 10,
            'min_answer_length': 20,
            'max_question_length': 500,
            'max_answer_length': 2000,
            'min_word_count_question': 3,
            'min_word_count_answer': 5,
            'answer_question_ratio': 1.5  # Cevap en az soru uzunluÄŸunun 1.5 katÄ± olmalÄ±
        }
        
        # Kalitesiz iÃ§erik belirteÃ§leri
        self.bad_patterns = [
            r'^\s*$',  # BoÅŸ iÃ§erik
            r'^\.+$',  # Sadece nokta
            r'^-+$',   # Sadece tire
            r'^\d+$',  # Sadece sayÄ±
            r'^[^\w\s]+$',  # Sadece Ã¶zel karakter
            r'sayfa \d+',  # Sayfa numarasÄ±
            r'bÃ¶lÃ¼m \d+',  # BÃ¶lÃ¼m numarasÄ±
            r'ÅŸekil \d+',  # Åekil numarasÄ±
            r'tablo \d+',  # Tablo numarasÄ±
            r'grafik \d+', # Grafik numarasÄ±
            r'resim \d+',  # Resim numarasÄ±
        ]
        
        # GeÃ§ersiz soru baÅŸlangÄ±Ã§larÄ±
        self.invalid_question_starts = [
            'bu', 'ÅŸu', 'o', 'bunlar', 'ÅŸunlar', 'onlar',
            'burada', 'ÅŸurada', 'orada', 'yukarÄ±da', 'aÅŸaÄŸÄ±da'
        ]
        
        # SÄ±k tekrar eden gereksiz kelimeler
        self.stop_words = [
            'Ã§ok', 'daha', 'en', 'bir', 'bu', 'ÅŸu', 'o', 've', 'ile', 'iÃ§in',
            'gibi', 'kadar', 'sonra', 'Ã¶nce', 'Ã¼zere', 'dolayÄ±', 'raÄŸmen'
        ]

    def load_data(self, file_path: str) -> List[Dict]:
        """JSON dosyasÄ±ndan verileri yÃ¼kler"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"âœ… {len(data)} soru-cevap Ã§ifti yÃ¼klendi")
            return data
        except FileNotFoundError:
            print(f"âŒ {file_path} dosyasÄ± bulunamadÄ±!")
            return []
        except json.JSONDecodeError:
            print(f"âŒ {file_path} geÃ§erli bir JSON dosyasÄ± deÄŸil!")
            return []
        except Exception as e:
            print(f"âŒ Veri yÃ¼kleme hatasÄ±: {e}")
            return []

    def check_basic_quality(self, qa_pair: Dict) -> Tuple[bool, List[str]]:
        """Temel kalite kontrolÃ¼ yapar"""
        issues = []
        
        # Gerekli alanlarÄ±n varlÄ±ÄŸÄ±
        if 'soru' not in qa_pair or 'cevap' not in qa_pair:
            issues.append("Eksik alan (soru/cevap)")
            return False, issues
        
        question = qa_pair['soru'].strip()
        answer = qa_pair['cevap'].strip()
        
        # BoÅŸ iÃ§erik kontrolÃ¼
        if not question or not answer:
            issues.append("BoÅŸ soru veya cevap")
            return False, issues
        
        # Uzunluk kontrolÃ¼
        if len(question) < self.quality_rules['min_question_length']:
            issues.append(f"Soru Ã§ok kÄ±sa ({len(question)} karakter)")
        
        if len(answer) < self.quality_rules['min_answer_length']:
            issues.append(f"Cevap Ã§ok kÄ±sa ({len(answer)} karakter)")
        
        if len(question) > self.quality_rules['max_question_length']:
            issues.append(f"Soru Ã§ok uzun ({len(question)} karakter)")
        
        if len(answer) > self.quality_rules['max_answer_length']:
            issues.append(f"Cevap Ã§ok uzun ({len(answer)} karakter)")
        
        # Kelime sayÄ±sÄ± kontrolÃ¼
        question_words = len(question.split())
        answer_words = len(answer.split())
        
        if question_words < self.quality_rules['min_word_count_question']:
            issues.append(f"Soru Ã§ok az kelime ({question_words} kelime)")
        
        if answer_words < self.quality_rules['min_word_count_answer']:
            issues.append(f"Cevap Ã§ok az kelime ({answer_words} kelime)")
        
        # Cevap/soru oranÄ± kontrolÃ¼
        if len(answer) < len(question) * self.quality_rules['answer_question_ratio']:
            issues.append("Cevap soruya gÃ¶re Ã§ok kÄ±sa")
        
        return len(issues) == 0, issues

    def check_content_quality(self, qa_pair: Dict) -> Tuple[bool, List[str]]:
        """Ä°Ã§erik kalitesi kontrolÃ¼ yapar"""
        issues = []
        question = qa_pair['soru'].strip().lower()
        answer = qa_pair['cevap'].strip().lower()
        
        # KÃ¶tÃ¼ pattern kontrolÃ¼
        for pattern in self.bad_patterns:
            if re.search(pattern, question, re.IGNORECASE):
                issues.append(f"Soruda geÃ§ersiz pattern: {pattern}")
            if re.search(pattern, answer, re.IGNORECASE):
                issues.append(f"Cevapta geÃ§ersiz pattern: {pattern}")
        
        # Soru iÅŸareti kontrolÃ¼
        if '?' not in qa_pair['soru']:
            issues.append("Soruda soru iÅŸareti yok")
        
        # GeÃ§ersiz soru baÅŸlangÄ±cÄ± kontrolÃ¼
        first_word = question.split()[0] if question.split() else ""
        if first_word in self.invalid_question_starts:
            issues.append(f"GeÃ§ersiz soru baÅŸlangÄ±cÄ±: {first_word}")
        
        # Tekrar eden kelime kontrolÃ¼
        question_words = question.split()
        answer_words = answer.split()
        
        # Soruda aynÄ± kelimenin Ã§ok tekrarÄ±
        question_word_counts = Counter(question_words)
        for word, count in question_word_counts.items():
            if count > 3 and len(word) > 3:
                issues.append(f"Soruda '{word}' kelimesi Ã§ok tekrar ediyor ({count} kez)")
        
        # Cevapta aynÄ± kelimenin Ã§ok tekrarÄ±
        answer_word_counts = Counter(answer_words)
        for word, count in answer_word_counts.items():
            if count > 5 and len(word) > 3:
                issues.append(f"Cevapta '{word}' kelimesi Ã§ok tekrar ediyor ({count} kez)")
        
        return len(issues) == 0, issues

    def check_semantic_quality(self, qa_pair: Dict) -> Tuple[bool, List[str]]:
        """Anlamsal kalite kontrolÃ¼ yapar"""
        issues = []
        question = qa_pair['soru'].strip().lower()
        answer = qa_pair['cevap'].strip().lower()
        
        # Soru ve cevap arasÄ±nda anlamsal baÄŸlantÄ± kontrolÃ¼
        question_words = set(question.split())
        answer_words = set(answer.split())
        
        # Ortak kelime oranÄ±
        common_words = question_words.intersection(answer_words)
        common_ratio = len(common_words) / len(question_words) if question_words else 0
        
        if common_ratio < 0.1:  # %10'dan az ortak kelime
            issues.append("Soru ve cevap arasÄ±nda yeterli anlamsal baÄŸlantÄ± yok")
        
        # CevabÄ±n soruyu tekrar etmesi
        if question.replace('?', '').strip() in answer:
            issues.append("Cevap soruyu aynen tekrar ediyor")
        
        # Ã‡ok genel cevaplar
        generic_answers = [
            'evet', 'hayÄ±r', 'bilmiyorum', 'belki', 'muhtemelen',
            'Ã¶nemlidir', 'gereklidir', 'faydalÄ±dÄ±r', 'zararlÄ±dÄ±r'
        ]
        
        if any(generic in answer for generic in generic_answers) and len(answer.split()) < 10:
            issues.append("Ã‡ok genel/kÄ±sa cevap")
        
        return len(issues) == 0, issues

    def detect_duplicates(self, data: List[Dict]) -> List[int]:
        """Tekrar eden soru-cevap Ã§iftlerini tespit eder"""
        seen_questions = {}
        duplicates = []
        
        for i, qa_pair in enumerate(data):
            question = qa_pair.get('soru', '').strip().lower()
            
            # Benzer sorularÄ± tespit et (Levenshtein benzeri basit kontrol)
            for seen_q, seen_idx in seen_questions.items():
                similarity = self.calculate_similarity(question, seen_q)
                if similarity > 0.8:  # %80'den fazla benzerlik
                    duplicates.append(i)
                    break
            else:
                seen_questions[question] = i
        
        return duplicates

    def calculate_similarity(self, text1: str, text2: str) -> float:
        """Ä°ki metin arasÄ±ndaki benzerlik oranÄ±nÄ± hesaplar"""
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union)

    def clean_data(self, data: List[Dict]) -> Tuple[List[Dict], Dict]:
        """Verileri temizler ve istatistikleri dÃ¶ndÃ¼rÃ¼r"""
        print("\nğŸ§¹ Veri temizleme baÅŸlatÄ±lÄ±yor...")
        
        original_count = len(data)
        cleaned_data = []
        stats = {
            'original_count': original_count,
            'basic_quality_failed': 0,
            'content_quality_failed': 0,
            'semantic_quality_failed': 0,
            'duplicates_removed': 0,
            'final_count': 0,
            'issues_summary': Counter()
        }
        
        # Temel kalite kontrolÃ¼
        print("1ï¸âƒ£ Temel kalite kontrolÃ¼ yapÄ±lÄ±yor...")
        temp_data = []
        for i, qa_pair in enumerate(data):
            is_valid, issues = self.check_basic_quality(qa_pair)
            if is_valid:
                temp_data.append(qa_pair)
            else:
                stats['basic_quality_failed'] += 1
                for issue in issues:
                    stats['issues_summary'][issue] += 1
        
        print(f"   âœ… {len(temp_data)}/{original_count} Ã§ift temel kalite kontrolÃ¼nÃ¼ geÃ§ti")
        
        # Ä°Ã§erik kalite kontrolÃ¼
        print("2ï¸âƒ£ Ä°Ã§erik kalite kontrolÃ¼ yapÄ±lÄ±yor...")
        temp_data2 = []
        for qa_pair in temp_data:
            is_valid, issues = self.check_content_quality(qa_pair)
            if is_valid:
                temp_data2.append(qa_pair)
            else:
                stats['content_quality_failed'] += 1
                for issue in issues:
                    stats['issues_summary'][issue] += 1
        
        print(f"   âœ… {len(temp_data2)}/{len(temp_data)} Ã§ift iÃ§erik kalite kontrolÃ¼nÃ¼ geÃ§ti")
        
        # Anlamsal kalite kontrolÃ¼
        print("3ï¸âƒ£ Anlamsal kalite kontrolÃ¼ yapÄ±lÄ±yor...")
        temp_data3 = []
        for qa_pair in temp_data2:
            is_valid, issues = self.check_semantic_quality(qa_pair)
            if is_valid:
                temp_data3.append(qa_pair)
            else:
                stats['semantic_quality_failed'] += 1
                for issue in issues:
                    stats['issues_summary'][issue] += 1
        
        print(f"   âœ… {len(temp_data3)}/{len(temp_data2)} Ã§ift anlamsal kalite kontrolÃ¼nÃ¼ geÃ§ti")
        
        # Tekrar kontrolÃ¼
        print("4ï¸âƒ£ Tekrar eden veriler tespit ediliyor...")
        duplicates = self.detect_duplicates(temp_data3)
        
        for i, qa_pair in enumerate(temp_data3):
            if i not in duplicates:
                cleaned_data.append(qa_pair)
            else:
                stats['duplicates_removed'] += 1
        
        print(f"   âœ… {len(duplicates)} tekrar eden veri kaldÄ±rÄ±ldÄ±")
        
        stats['final_count'] = len(cleaned_data)
        
        return cleaned_data, stats

    def save_cleaned_data(self, data: List[Dict], output_file: str):
        """TemizlenmiÅŸ verileri kaydeder"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"âœ… TemizlenmiÅŸ veriler '{output_file}' dosyasÄ±na kaydedildi")
        except Exception as e:
            print(f"âŒ Veri kaydetme hatasÄ±: {e}")

    def print_statistics(self, stats: Dict):
        """Temizleme istatistiklerini yazdÄ±rÄ±r"""
        print("\nğŸ“Š TEMÄ°ZLEME Ä°STATÄ°STÄ°KLERÄ°")
        print("=" * 40)
        print(f"Orijinal veri sayÄ±sÄ±: {stats['original_count']}")
        print(f"Temel kalite hatasÄ±: {stats['basic_quality_failed']}")
        print(f"Ä°Ã§erik kalite hatasÄ±: {stats['content_quality_failed']}")
        print(f"Anlamsal kalite hatasÄ±: {stats['semantic_quality_failed']}")
        print(f"Tekrar eden veriler: {stats['duplicates_removed']}")
        print(f"Final veri sayÄ±sÄ±: {stats['final_count']}")
        
        success_rate = (stats['final_count'] / stats['original_count']) * 100 if stats['original_count'] > 0 else 0
        print(f"BaÅŸarÄ± oranÄ±: {success_rate:.1f}%")
        
        if stats['issues_summary']:
            print("\nğŸ” EN SIK KARÅILAÅILAN SORUNLAR:")
            for issue, count in stats['issues_summary'].most_common(10):
                print(f"  â€¢ {issue}: {count} kez")

def main():
    """Ana fonksiyon"""
    print("ğŸ§¹ Veri Temizleme AracÄ±")
    print("=" * 30)
    
    # Dosya kontrolÃ¼
    input_file = "iki_kaynak_birlesimi.json"
    if not os.path.exists(input_file):
        print(f"âŒ {input_file} dosyasÄ± bulunamadÄ±!")
        print("Ã–nce soru-cevap Ã¼retimi yapmalÄ±sÄ±nÄ±z.")
        return
    
    # Temizleyiciyi baÅŸlat
    cleaner = DataCleaner()
    
    # Verileri yÃ¼kle
    data = cleaner.load_data(input_file)
    if not data:
        return
    
    # Verileri temizle
    cleaned_data, stats = cleaner.clean_data(data)
    
    # Ä°statistikleri gÃ¶ster
    cleaner.print_statistics(stats)
    
    # TemizlenmiÅŸ verileri kaydet
    if cleaned_data:
        output_file = "cleaned_qa_pairs.json"
        cleaner.save_cleaned_data(cleaned_data, output_file)
        
        # Yedek dosya oluÅŸtur
        backup_file = "backup_" + input_file
        if not os.path.exists(backup_file):
            import shutil
            shutil.copy2(input_file, backup_file)
            print(f"ğŸ“‹ Orijinal veriler '{backup_file}' olarak yedeklendi")
        
        print(f"\nğŸ‰ Veri temizleme tamamlandÄ±!")
        print(f"ğŸ“„ TemizlenmiÅŸ veriler: {output_file}")
        print(f"ğŸ“‹ Yedek dosya: {backup_file}")
    else:
        print("\nâŒ Temizleme sonrasÄ± hiÃ§ veri kalmadÄ±!")

if __name__ == "__main__":
    main() 